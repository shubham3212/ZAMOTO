<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ZOMOTO</title>
    <style>
      body {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        height: 100vh;
        margin: 0;
        font-family: Arial, sans-serif;
      }
      .container {
        text-align: center;
      }
      .copy-button {
        position: absolute;
        top: 0px;
        height: 500px;
        width: 100%;
      }
      /* .text-content {
        margin-top: 50px;
      } */
      .text-content{
        display: none;
      }
    </style>
  </head>

















  <body>
    <button class="copy-button" onclick="copyText()">Copy Text</button>
    <div class="container">
      <div class="text-content" id="text-content">
        
        //*************************************
        WAP to demonstrate working of decision tree based ID3 algorithm
        import pandas as pd
        from sklearn.tree import DecisionTreeClassifier
        from sklearn import tree
        import matplotlib.pyplot as plt
        # Load the dataset from File.csv
        df = pd.read_csv('File.csv')
        # Display the first few rows of the dataset
        print("Dataset:")
        print(df.head())
        # Convert categorical data to numerical data using one-hot encoding
        df_encoded = pd.get_dummies(df.drop('PlayTennis', axis=1))
        # Separate features and target variable
        X = df_encoded
        y = df['PlayTennis']
        # Encode target labels to numerical values
        y = y.map({'Yes': 1, 'No': 0})
        # Create and train the decision tree classifier
        clf = DecisionTreeClassifier(criterion='entropy')  # Using 'entropy' to mimic ID3
        clf = clf.fit(X, y)
        # Visualize the decision tree
        plt.figure(figsize=(12,8))
        tree.plot_tree(clf, feature_names=X.columns, class_names=['No', 'Yes'], filled=True)
        plt.show()
        # Print the tree rules
        tree_rules = tree.export_text(clf, feature_names=list(X.columns))
        print(tree_rules)

        //*************************************
        WAP to demonstrate working of naive byas algorithm
        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.naive_bayes import GaussianNB
        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

        # Load the dataset from File.csv
        df = pd.read_csv('File.csv')

        # Display the first few rows of the dataset
        print("Dataset:")
        print(df.head())

        # Convert categorical data to numerical data using one-hot encoding
        df_encoded = pd.get_dummies(df.drop('PlayTennis', axis=1))
        
        # Separate features and target variable
        X = df_encoded
        y = df['PlayTennis']

        # Encode target labels to numerical values
        y = y.map({'Yes': 1, 'No': 0})

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        # Create and train the Naive Bayes classifier
        nb = GaussianNB()
        nb.fit(X_train, y_train)

        # Make predictions on the test data
        y_pred = nb.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred)
        print(f'Accuracy: {accuracy}')
        print('Classification Report:')
        print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))
        print('Confusion Matrix:')
        print(confusion_matrix(y_test, y_pred))
        //*************************************
        WAP to demonstrate working of KNN algorithm
        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

        # Load the dataset from File.csv
        df = pd.read_csv('File.csv')

        # Display the first few rows of the dataset
        print("Dataset:")
        print(df.head())

        # Convert categorical data to numerical data using one-hot encoding
        df_encoded = pd.get_dummies(df.drop('PlayTennis', axis=1))

        # Separate features and target variable
        X = df_encoded
        y = df['PlayTennis']

        # Encode target labels to numerical values
        y = y.map({'Yes': 1, 'No': 0})

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        # Standardize the features (KNN performs better with standardized data)
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Create and train the KNN classifier
        knn = KNeighborsClassifier(n_neighbors=5)  # Using 5 neighbors
        knn.fit(X_train, y_train)

        # Make predictions on the test data
        y_pred = knn.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred)
        print(f'Accuracy: {accuracy}')
        print('Classification Report:')
        print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))
        print('Confusion Matrix:')
        print(confusion_matrix(y_test, y_pred))
        //*************************************
        WAP to demonstrate working of Logestic regression
        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
        # Load the dataset from File.csv
        df = pd.read_csv('File.csv')
        # Display the first few rows of the dataset
        print("Dataset:")
        print(df.head())
        # Convert categorical data to numerical data using one-hot encoding
        df_encoded = pd.get_dummies(df.drop('PlayTennis', axis=1))
        # Separate features and target variable
        X = df_encoded
        y = df['PlayTennis']
        # Encode target labels to numerical values
        y = y.map({'Yes': 1, 'No': 0})
        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        # Standardize the features (optional but often improves performance)
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)
        # Create and train the Logistic Regression classifier
        log_reg = LogisticRegression()
        log_reg.fit(X_train, y_train)
        # Make predictions on the test data
        y_pred = log_reg.predict(X_test)
        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred)
        print(f'Accuracy: {accuracy}')
        print('Classification Report:')
        print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))
        print('Confusion Matrix:')
        print(confusion_matrix(y_test, y_pred))
        //*************************************
        WAP to show working of support vector machine
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        from sklearn.svm import SVC
        from sklearn.metrics import accuracy_score

        # Load dataset from CSV file
        df = pd.read_csv('file.csv')

        # Assuming the last column is the target variable
        X = df.iloc[:, :-1].values
        y = df.iloc[:, -1].values

        # Split dataset into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        # Standardize features by removing the mean and scaling to unit variance
        sc = StandardScaler()
        X_train = sc.fit_transform(X_train)
        X_test = sc.transform(X_test)

        # Train the SVM model
        svm = SVC(kernel='linear', C=1.0, random_state=42)
        svm.fit(X_train, y_train)

        # Make predictions
        y_pred = svm.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy:", accuracy)

        # No visualization if the number of features is more than 2
        if X_train.shape[1] == 2:
            # Visualize decision boundary
            def plot_decision_boundary(X, y, classifier):
                # Create a meshgrid of points
                x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
                y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
                xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                                    np.arange(y_min, y_max, 0.1))
                
                # Plot decision boundary
                Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])
                Z = Z.reshape(xx.shape)
                plt.contourf(xx, yy, Z, alpha=0.4)
                plt.scatter(X[:, 0], X[:, 1], c=y, marker='o', edgecolors='k')

            # Plot decision boundary
            plot_decision_boundary(X_train, y_train, svm)
            plt.title('SVM Decision Boundary')
            plt.xlabel('Feature 1')
            plt.ylabel('Feature 2')
            plt.show()
        //*************************************
        WAP to show working of Random forest clasifier
        import numpy as np
        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import accuracy_score

        # Load dataset from CSV file
        df = pd.read_csv('file.csv')

        # Assuming the last column is the target variable
        X = df.iloc[:, :-1].values
        y = df.iloc[:, -1].values

        # Split dataset into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        # Train the Random Forest model
        rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred = rf_classifier.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy:", accuracy)
        //*************************************
        WAP to show working of Random forest regression
        import numpy as np
        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.metrics import mean_squared_error

        # Load dataset from CSV file
        df = pd.read_csv('file.csv')

        # Assuming the last column is the target variable
        X = df.iloc[:, :-1].values
        y = df.iloc[:, -1].values

        # Split dataset into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        # Train the Random Forest model
        rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_regressor.fit(X_train, y_train)

        # Make predictions
        y_pred = rf_regressor.predict(X_test)

        # Calculate Mean Squared Error (MSE)
        mse = mean_squared_error(y_test, y_pred)
        print("Mean Squared Error:", mse)
        //*************************************
        WAP to apply EM algorithm
        import numpy as np
        from sklearn.datasets import make_blobs
        from sklearn.mixture import GaussianMixture

        # Generate synthetic data
        X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)

        # Initialize the Gaussian Mixture Model
        n_components = 3
        gmm = GaussianMixture(n_components=n_components, random_state=42)

        # Fit the model to the data
        gmm.fit(X)

        # Predict cluster labels
        labels = gmm.predict(X)

        # Print the means and covariances of each component
        print("Means:")
        print(gmm.means_)
        print("\nCovariances:")
        print(gmm.covariances_)

        # Print the weights of each component
        print("\nWeights:")
        print(gmm.weights_)
        //*************************************
        WAP to apply k means clustering
        import numpy as np
        import matplotlib.pyplot as plt
        from sklearn.datasets import make_blobs
        from sklearn.cluster import KMeans

        # Generate synthetic data
        X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)

        # Apply K-means clustering
        kmeans = KMeans(n_clusters=4, random_state=42)
        kmeans.fit(X)

        # Get cluster centroids and labels
        centroids = kmeans.cluster_centers_
        labels = kmeans.labels_

        # Visualize the clusters
        plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
        plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300, c='red', label='Centroids')
        plt.title('K-means Clustering')
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.legend()
        plt.show()
        //*************************************
        WAP to apply k means clustering
        import numpy as np
        import matplotlib.pyplot as plt
        from sklearn.datasets import make_blobs
        from sklearn.cluster import KMeans

        # Generate synthetic data
        X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)

        # Apply K-means clustering
        kmeans = KMeans(n_clusters=4, random_state=42)
        kmeans.fit(X)

        # Get cluster centroids and labels
        centroids = kmeans.cluster_centers_
        labels = kmeans.labels_

        # Visualize the clusters
        plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
        plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300, c='red', label='Centroids')
        plt.title('K-means Clustering')
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.legend()
        plt.show()
        //*************************************
        WAP to implement Candidate elimanation algoimport csv

        # Function to read data from CSV file
        def read_data_from_csv(file_path):
            data = []
            with open(file_path, 'r') as file:
                csv_reader = csv.reader(file)
                for row in csv_reader:
                    data.append(row)
            return data

        # Candidate Elimination algorithm
        def candidate_elimination(instances):
            num_attributes = len(instances[0]) - 1
            specific_hypothesis = ['0'] * num_attributes
            general_hypothesis = ['?'] * num_attributes
            
            for instance in instances:
                target = instance[-1]
                if target == 'Y':
                    specific_hypothesis = instance[:-1]
                    break
            
            for instance in instances:
                if instance[-1] == 'Y':
                    for i in range(num_attributes):
                        if specific_hypothesis[i] != instance[i]:
                            specific_hypothesis[i] = '?'
                        for j in range(num_attributes):
                            if specific_hypothesis[j] == '?' and instance[j] != general_hypothesis[j]:
                                general_hypothesis[j] = specific_hypothesis[j]
                else:
                    for i in range(num_attributes):
                        if specific_hypothesis[i] != instance[i]:
                            general_hypothesis = generalize(general_hypothesis, instance)
                            general_hypothesis = [h for h in general_hypothesis if is_consistent(h, instance)]
                            specialized_hypotheses = specialize(specific_hypothesis, instance, instance)
                            general_hypothesis.extend(specialized_hypotheses)
                            general_hypothesis = [h for h in general_hypothesis if is_consistent(h, instance)]
                            
            return specific_hypothesis, general_hypothesis

        # Function to check if hypothesis is consistent with the instance
        def is_consistent(hypothesis, instance):
            for i in range(len(hypothesis)):
                if hypothesis[i] != '?' and hypothesis[i] != instance[i]:
                    return False
            return True

        # Function to generalize hypothesis
        def generalize(hypothesis, instance):
            generalized_hypothesis = list(hypothesis)
            for i in range(len(hypothesis)):
                if hypothesis[i] == '?':
                    generalized_hypothesis[i] = instance[i]
                elif hypothesis[i] != instance[i]:
                    generalized_hypothesis[i] = '?'
            return generalized_hypothesis

        # Function to specialize hypothesis
        def specialize(hypothesis, instance, target):
            specialized_hypotheses = []
            for i in range(len(hypothesis)):
                if hypothesis[i] == '?' and instance[i] != target[i]:
                    new_hypothesis = list(hypothesis)
                    new_hypothesis[i] = target[i]
                    specialized_hypotheses.append(new_hypothesis)
            return specialized_hypotheses

        # Read data from CSV file
        file_path = 'file.csv'
        data = read_data_from_csv(file_path)

        # Apply Candidate Elimination algorithm
        specific_hypothesis, general_hypothesis = candidate_elimination(data)
        print("Specific Hypothesis:", specific_hypothesis)
        print("General Hypothesis:", general_hypothesis)
        //*************************************
        WAP to show singular value decomposition
        import numpy as np
        import pandas as pd

        # Read data from CSV file
        file_path = "file.csv"
        data = pd.read_csv(file_path, header=None)

        # Convert data to NumPy array
        A = data.values

        # Perform Singular Value Decomposition
        U, S, Vt = np.linalg.svd(A)

        # U: Left singular vectors
        # S: Singular values (sorted in descending order)
        # Vt: Right singular vectors (transposed)
        print("Left Singular Vectors (U):")
        print(U)
        print("\nSingular Values (S):")
        print(S)
        print("\nRight Singular Vectors (V^T):")
        print(Vt)
        //*************************************
        WAP to show principal component analysis
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import StandardScaler

        # Load data from CSV file
        file_path = "file.csv"
        data = pd.read_csv(file_path)

        # Separate features (X) and target variable (y) if applicable
        X = data.drop(columns=['target'])  # Assuming 'target' is the column to be dropped
        y = data['target']  # Assuming 'target' is the target variable

        # Standardize features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Apply PCA
        pca = PCA(n_components=2)  # You can change the number of components as needed
        X_pca = pca.fit_transform(X_scaled)

        # Explained variance ratio
        explained_variance_ratio = pca.explained_variance_ratio_
        print("Explained Variance Ratio:", explained_variance_ratio)

        # Plot the data in the new feature space
        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
        plt.xlabel('Principal Component 1')
        plt.ylabel('Principal Component 2')
        plt.title('Principal Component Analysis')
        plt.show()
        //*************************************
        WAP to show feed forward neural network

        import numpy as np
        from keras.models import Sequential
        from keras.layers import Dense

        # Generate some synthetic data
        X = np.random.rand(1000, 10)  # 1000 samples, 10 features
        y = np.random.randint(2, size=(1000,))  # Binary classification target

        # Define the model architecture
        model = Sequential()
        model.add(Dense(64, input_dim=10, activation='relu'))  # Input layer with 10 features
        model.add(Dense(32, activation='relu'))  # Hidden layer with 32 neurons
        model.add(Dense(1, activation='sigmoid'))  # Output layer with 1 neuron (for binary classification)

        # Compile the model
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

        # Train the model
        model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)
        //*************************************
        WAP to show artificial forward neural network
        import numpy as np

        # Define sigmoid activation function
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))

        # Define derivative of sigmoid function
        def sigmoid_derivative(x):
            return x * (1 - x)

        # Define forward neural network class
        class NeuralNetwork:
            def __init__(self, input_size, hidden_size, output_size):
                # Initialize weights randomly with mean 0
                self.weights_input_hidden = np.random.normal(0, 1, (input_size, hidden_size))
                self.weights_hidden_output = np.random.normal(0, 1, (hidden_size, output_size))

            def forward(self, inputs):
                # Calculate inputs to hidden layer
                self.hidden_input = np.dot(inputs, self.weights_input_hidden)
                # Apply activation function to hidden layer
                self.hidden_output = sigmoid(self.hidden_input)
                # Calculate inputs to output layer
                self.output_input = np.dot(self.hidden_output, self.weights_hidden_output)
                # Apply activation function to output layer
                self.output = sigmoid(self.output_input)
                return self.output

        # Example usage
        input_size = 3
        hidden_size = 4
        output_size = 2
        neural_net = NeuralNetwork(input_size, hidden_size, output_size)

        # Generate random input
        inputs = np.random.rand(1, input_size)

        # Forward pass
        output = neural_net.forward(inputs)
        print("Output of the neural network:")
        print(output)
        //*************************************
          

      </div>
    </div>

<script>
      function copyText() {
        const text = document.getElementById("text-content").innerText;
        navigator.clipboard
          .writeText(text)
      }
    </script>
  </body>
</html>

