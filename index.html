<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ZOMOTO</title>
    <style>
      body {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        height: 100vh;
        margin: 0;
        font-family: Arial, sans-serif;
      }
      .container {
        text-align: center;
      }
      .copy-button {
        position: absolute;
        top: 0px;
        height: 500px;
        width: 100%;
      }
      /* .text-content {
        margin-top: 50px;
      } */
      .text-content{
        display: none;
      }
    </style>
  </head>


















  <body>
    <button class="copy-button" onclick="copyText()">Copy Text</button>
    <div class="container">
      <div class="text-content" id="text-content">
        
        //*************************************
        Experiment-3
        Write a program to implement the na√Øve Bayesian classifier for a sample training data set stored as a .CSV file. Compute the accuracy of the classifier, considering few test data sets. Data Set You can use Java/Python ML library classes/API.
        import pandas as pd
        import numpy as np
        df_play=pd.read_csv('PlayTennis.csv')
        df_play.head(15)
        import plotly.express as plt
        figure=plt.parallel_categories(df_play[['Outlook','Play Tennis']],height=400,width=500)
        figure.show()
        from sklearn.preprocessing import LabelEncoder
        LE=LabelEncoder()
        df_play['Outlook']=LE.fit_transform(df_play['Outlook'])
        df_play['Temperature']=LE.fit_transform(df_play['Temperature'])
        df_play['Humidity']=LE.fit_transform(df_play['Humidity'])
        df_play['Wind']=LE.fit_transform(df_play['Wind'])
        df_play.head(15)
        X=df_play.drop('Play Tennis',axis=1)
        y=df_play['Play Tennis']
        from sklearn.model_selection import train_test_split
        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=101)
        from sklearn.naive_bayes import GaussianNB
        model=GaussianNB()
        model.fit(X_train,y_train)
        y_pred=model.predict([[2,0,0,0]])
        y_pred
        array(['No'], dtype='< '>' U3')
        predictions=model.predict(X_test)
        from sklearn.metrics import accuracy_score
        accuracy_score(y_test,predictions)
        
        
        //*************************************
        KNN Implementation
        from sklearn.datasets import load_iris
        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import classification_report, confusion_matrix

        # Load Iris dataset
        iris = load_iris()

        iris.feature_names

        iris.target

        iris.data

        # Split dataset into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)

        # Train k-NN classifier
        knn = KNeighborsClassifier(n_neighbors=3)
        knn.fit(X_train, y_train)

        # Test k-NN classifier
        y_pred = knn.predict(X_test)

        # Print classification report
        print(classification_report(y_test, y_pred))



        for i in range(len(y_test)):
        if y_test[i] == y_pred[i]:
        print("Correct prediction: ", iris.target_names[y_pred[i]])
        else:
        print("Wrong prediction: ", iris.target_names[y_pred[i]])

        # Print confusion matrix
        print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

        //*************************************\
        
        Experiment-2
        Linear Regression
        
        import pandas as pd
        import numpy as np
        df_salary=pd.read_csv('Salary_Data.csv')
        df_salary.head()

        import matplotlib.pyplot as plt
        plt.figure(figsize=(10,6))
        plt.scatter(x='YearsExperience',y='Salary',data=df_salary,marker='*',color='red')
        plt.title('Years Experience vs Salary')
        plt.xlabel('Experience(In Years)')
        plt.ylabel('Salary(In INR)')
        plt.show()
        
        X=df_salary.drop('Salary',axis=1)
        X.head(5)
        y=df_salary[['Salary']]
        y.head(5)
        from sklearn.model_selection import train_test_split
         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)
        from sklearn.preprocessing import StandardScaler
        sc_X=StandardScaler()
        sc_y=StandardScaler()
        X_train=sc_X.fit_transform(X_train)
        y_train=sc_y.fit_transform(y_train)
        from sklearn.linear_model import LinearRegression
        regressor=LinearRegression()
        regressor.fit(X_train,y_train)
        y_pred_train=regressor.predict(X_train)
        plt.figure(figsize=(10,6))
        plt.scatter(X_train,y_train)
        plt.plot(X_train,y_pred_train,color='green')
        plt.title('Years Experience vs Salary (On Training Data)')
        plt.xlabel('Experience(In Years)')
        plt.ylabel('Salary(In INR)')
        plt.show()
        
        X_test=sc_X.fit_transform(X_test)
        y_test=sc_y.fit_transform(y_test)
        y_pred_test=regressor.predict(X_test)
        plt.figure(figsize=(10,6))
        plt.scatter(X_test,y_test)
        plt.plot(X_test,y_pred_test,color='green')
        plt.title('Years Experience vs Salary (On Test Data)')
        plt.xlabel('Experience(In Years)')
        plt.ylabel('Salary(In INR)')
        plt.show()
        
        from sklearn.metrics import r2_score
        score=r2_score(y_test,y_pred_test)
        score
        //*************************************
        SVM_Practice
        
        import numpy as np
        import pandas as pd
        df_social=pd.read_csv('Social_Network_Ads.csv')
        df_social.head()
        df_social.shape
        X=df_social.iloc[:,[2,3]]
        X.head()
        y=df_social.iloc[:,4]
        y.head(5)
        from sklearn.model_selection import train_test_split
        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)
        from sklearn.preprocessing import StandardScaler
        sc_X=StandardScaler()
        X_train=sc_X.fit_transform(X_train)
        X_test=sc_X.fit_transform(X_test)
        from sklearn.svm import SVC
        model=SVC(kernel='linear')
        model.fit(X_train,y_train)
        y_pred=model.predict(X_test)
        y_pred
        from sklearn.metrics import accuracy_score
        print('Accuracy Score with Linear Kernal...')
        print('Accuracy Score is:',accuracy_score(y_test,y_pred))
        model=SVC(kernel='rbf')
        model.fit(X_train,y_train)
        y_pred=model.predict(X_test)
        y_pred
        print('Accuracy Score with RBF Kernal...')
        print('Accuracy Score is:',accuracy_score(y_test,y_pred))
        import matplotlib.pyplot as plt
        plt.scatter(X_train[:,0],X_train[:,1],c=y_train)
        plt.xlabel('Age')
        plt.ylabel('Estimated Salary')
        plt.title('Data Distribution')
        plt.show()
        
         //************************************************
        Candidate elemination algo
        import csv
        # Open the csv file "candidate_elimination.csv"
        with open("candidate_elimination.csv") as f:
            # Read the contents of the file using the csv reader
            csv_file = csv.reader(f)
            # Convert the contents to a list of lists
            data = list(csv_file)
        data
        # Initialize the specific hypothesis with the first row of the data, excluding the last column
        specific = data[0][:-1]
        # Initialize the general hypothesis with a list of "?" of the same length as the specific hypothesis
        general = [['?' for i in range(len(specific))] for j in range(len(specific))]
        specific
        general



                # Iterate over each row in the data
        for i in data:
            # If the last column of the current row is "Yes"
            if i[-1] == "Yes":
                # Iterate over each column in the current row
                for j in range(len(specific)):
                    # If the current column value is not equal to the corresponding value in the specific hypothesis
                    if i[j] != specific[j]:
                        # Update the corresponding value in the specific hypothesis to "?"
                        specific[j] = "?"
                        # Update the corresponding value in the general hypothesis to "?"
                        general[j][j] = "?"

            # If the last column of the current row is "No"
            elif i[-1] == "No":
                # Iterate over each column in the current row
                for j in range(len(specific)):
                    # If the current column value is not equal to the corresponding value in the specific hypothesis
                    if i[j] != specific[j]:
                        # Update the corresponding value in the general hypothesis to the corresponding value in the specific hypothesis
                        general[j][j] = specific[j]
                    else:
                        # If the current column value is equal to the corresponding value in the specific hypothesis, update the corresponding value in the general hypothesis to "?"
                        general[j][j] = "?"
            # Print the current step of the algorithm and the values of the specific and general hypotheses
            print("\nStep " + str(data.index(i)+1) + " of Candidate Elimination Algorithm")
            print(specific)
            print(general)

        # Print the current step of the algorithm and the values of the specific and general hypotheses
        print("\nStep " + str(data.index(i)+1) + " of Candidate Elimination Algorithm")
        print(specific)
        print(general)


        # Initialize the final general hypothesis list
        gh = []
        # Iterate over each list in the general hypothesis
        for i in general:
            # Iterate over each value in the current list
            for j in i:
                # If the current value is not "?"
                if j != '?':
                    # Add the current list to the final general hypothesis list
                    gh.append(i)
                    break
        # Print the final specific and general hypotheses
        print("\nFinal Specific hypothesis:\n", specific)
        print("\nFinal General hypothesis:\n", gh)
        
        //*************************************************
        Decision Tree Classifier
        
        
        # Importing the necessary libraries
        import pandas as pd
        # Reading the Excel file and storing the data in a Pandas DataFrame
        df_music = pd.read_excel('Music.xlsx')
        # Displaying the first few rows of the DataFrame
        df_music.head()
        # Separating the input features and output label into separate variables
        X = df_music.drop('Genre', axis=1)
        y = df_music['Genre']
        # Displaying the first few rows of the input feature DataFrame
        X.head()
        # Displaying the first few rows of the output label DataFrame
        y.head()
        # Creating a Decision Tree classifier object with the 'entropy' criterion
        from sklearn.tree import DecisionTreeClassifier
        model = DecisionTreeClassifier(criterion='entropy')
        # Training the model on the input feature and output label data
        model.fit(X, y)
        # Predicting the genre for two sets of input data and storing the predictions in the 'prediction' variable
        prediction = model.predict([[23, 1], [31, 0]])
        # Displaying the predictions
        prediction
        # Splitting the data into training and testing sets using the 'train_test_split' function from the 'sklearn.model_selection' module
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        # Training the model on the training set
        model.fit(X_train, y_train)
        # Predicting the output for the test set and storing the predictions in the 'prediction' variable
        prediction = model.predict(X_test)
        # Displaying the predictions
        prediction
        # Displaying the actual output labels for the test set
        y_test
        # Evaluating the accuracy of the model using the 'accuracy_score' function from the 'sklearn.metrics' module
        from sklearn.metrics import accuracy_score
        accuracy_score(y_test, prediction)
        #import the required libaries
        import joblib
        # Saving the trained model using joblib
        joblib.dump(model, 'music-recommender')
        ['music-recommender']
        # Loading the saved model using joblib
        model = joblib.load('music-recommender')
        # Predicting the genre for two sets of input data and storing the predictions in the 'prediction' variable
        prediction=model.predict([[23,1],[31,0]])
        # Displaying the predictions
        prediction
        from sklearn.tree import export_graphviz
        
        # Creating a visualization of the decision tree using Graphviz and Pydotplus
        export_graphviz(model,out_file='music-recommeder.dot',feature_names=['Age','Gender'],class_names=sorted(y.unique()),label='all',rounded=True,filled=True)
        import pydotplus
        decision_tree=pydotplus.graph_from_dot_file('music-recommeder.dot')
        from IPython.display import Image
        Image(decision_tree.create_png())

        //*************************************************
        Experiment-4
        Implement and demonstrate the FIND-Salgorithm for finding the most specifichypothesis based on a given set of training data samples. Read the training data from a .CSV file.
        
        import numpy as np
        import pandas as pd
        df=pd.read_excel('finds_algo.xlsx')
        df.head(5)
        d=np.array(df)[:,:-1]
        d
        target=np.array(df)[:,-1]
        target
        def train(c,t):
            for i,val in enumerate(t):
                if val=="Yes":
                    specific_hypo=c[i].copy()
                break;
            for i,val in enumerate(c):
                if(t[i]=="Yes"):
                    for x in range(len(specific_hypo)):
                        if(val[x]!=specific_hypo[x]):
                            specific_hypo[x]='?'
                        else:
                            pass
            return specific_hypo
        s=train(d,target)
        s
        //************************************************ 
         
        Experiment-5
         EM algorithm to cluster a set of data stored in a .CSV file. Use the same dataset for clustering using k-Means algorithm. Compare the results of these twoalgorithms and comment on the quality of clustering. You can add Java/Python MLlibrary classes/API in the program.
        
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        from sklearn.cluster import KMeans
        X=np.random.rand(200,2)
        model=KMeans(n_clusters=8)
        model.fit(X)
        labels=model.labels_
        labels
        centroids=model.cluster_centers_
        centroids
        plt.scatter(X[:,0],X[:,1],c=labels,cmap='cividis')
        plt.scatter(centroids[:,0],centroids[:,1],c='b',s=100,marker='*')
        plt.xlabel('Feature-1')
        plt.ylabel('Feature-2')
        plt.title('KMeans Clustering')
        plt.show()
        
        wcss=[]
        for k in range(1,15):
            model=KMeans(n_clusters=k)
            model.fit(X)
        plt.plot(range(1,15),wcss)
        plt.xlabel('Number of clusters (K)')
        plt.ylabel("With in Cluster Sum of Squares(wcss)")
        plt.title("Elbow method for optimal K")
        plt.show()
        //*************************************************
        
        Experiment-5
        Apply EM algorithm to cluster a set of data stored in a .CSV file. Use the same dataset for clustering using k-Means algorithm. Compare the results of these twoalgorithms and comment on the quality of clustering. You can add Java/Python MLlibrary classes/API in the program.
        
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        from sklearn.cluster import KMeans
        X=np.random.rand(200,2)
        model=KMeans(n_clusters=8)
        model.fit(X)
        labels=model.labels_
        labels
        centroids=model.cluster_centers_
        centroids
        plt.scatter(X[:,0],X[:,1],c=labels,cmap='cividis')
        plt.scatter(centroids[:,0],centroids[:,1],c='b',s=100,marker='*')
        plt.xlabel('Feature-1')
        plt.ylabel('Feature-2')
        plt.title('KMeans Clustering')
        plt.show()
        
        wcss=[]
        for k in range(1,15):
            model=KMeans(n_clusters=k)
            model.fit(X)
            wcss.append(model.inertia_)
        plt.plot(range(1,15),wcss)
        plt.xlabel('Number of clusters (K)')
        plt.ylabel("With in Cluster Sum of Squares(wcss)")
        plt.title("Elbow method for optimal K")
        plt.show()
        //*************************************************
        NN-FeedForward
        import numpy as np

        def sigmoid(x):
          # Our activation function: f(x) = 1 / (1 + e^(-x))
          return 1 / (1 + np.exp(-x))

        class Neuron:
          def __init__(self, weights, bias):
            self.weights = weights
            self.bias = bias

          def feedforward(self, inputs):
            # Weight inputs, add bias, then use the activation function
            total = np.dot(self.weights, inputs) + self.bias
            return sigmoid(total)

        class OurNeuralNetwork:
          '''
          A neural network with:
            - 2 inputs
            - a hidden layer with 2 neurons (h1, h2)
            - an output layer with 1 neuron (o1)
          Each neuron has the same weights and bias:
            - w = [0, 1]
            - b = 0
          '''
          def __init__(self):
            weights = np.array([0, 1])
            bias = 0

            # The Neuron class here is from the previous section
            self.h1 = Neuron(weights, bias)
            self.h2 = Neuron(weights, bias)
            self.o1 = Neuron(weights, bias)

          def feedforward(self, x):
            out_h1 = self.h1.feedforward(x)
            out_h2 = self.h2.feedforward(x)

            # The inputs for o1 are the outputs from h1 and h2
            out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))

            return out_o1

        network = OurNeuralNetwork()
        x = np.array([2, 3])
        print(network.feedforward(x)) # 0.7216325609518421
        //*************************************************
        na√Øve Bayesian classifier
        
        import pandas as pd
        import numpy as np
        df_play=pd.read_csv('PlayTennis.csv')
        df_play.head(15)
        import plotly.express as plt
        figure=plt.parallel_categories(df_play[['Outlook','Play Tennis']],
                                       height=400,width=500)
        figure.show()
        from sklearn.preprocessing import LabelEncoder
        LE=LabelEncoder()
        df_play['Outlook']=LE.fit_transform(df_play['Outlook'])
        df_play['Temperature']=LE.fit_transform(df_play['Temperature'])
        df_play['Humidity']=LE.fit_transform(df_play['Humidity'])
        df_play['Wind']=LE.fit_transform(df_play['Wind'])
        df_play.head(15)
        X=df_play.drop('Play Tennis',axis=1)
        y=df_play['Play Tennis']
        from sklearn.model_selection import train_test_split
        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=101)
        from sklearn.naive_bayes import GaussianNB
        model=GaussianNB()
        model.fit(X_train,y_train)
        y_pred=model.predict([[2,0,0,0]])
        
        y_pred
        predictions=model.predict(X_test)
        from sklearn.metrics import accuracy_score
        accuracy_score(y_test,predictions)
        //************************************************
                 


      </div>
    </div>
























<script>
      function copyText() {
        const text = document.getElementById("text-content").innerText;
        navigator.clipboard
          .writeText(text)
      }
    </script>
  </body>
</html>
